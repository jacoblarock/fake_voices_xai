import networks
import classification
from feature_extraction import check_cache
from dataclasses import dataclass
import pickle
import os
from datetime import datetime
from keras._tf_keras.keras import Model
import pandas as pd
import numpy as np
import lime.lime_tabular as lt
import tensorflow as tf
from random import uniform as rand

def gen_intermediate_train_data(model: Model,
                                features: list[pd.DataFrame],
                                feature_names: list[str],
                                batch_size: int
                                ) -> dict[str, pd.Series]:
    """
    Decomposes the input model (see networks.decompose) into its component models and returns the
    results of predictions of the so that independent explanations of the terminus slice can be
    made, allowing to draw a correlation of feature importance using the tabular explainer.
    Arguments:
    - model: composite model as generated using the function in networks
    - features: list of feature dataframes.
    - feature_names: list of feature name strings
    - batch_size: maximum batch size of the intermediate feature generation
    """
    out = {}
    dec_model = networks.decompose(model)
    # remove the terminus model
    if "terminus" in dec_model.keys():
        del dec_model["terminus"]
    if len(dec_model) != len(features):
        print(len(dec_model))
        print(len(features))
        raise ValueError("number of features does not correspond to the number of sub models")
    print()
    print()
    for i in range(len(features)):
        feature_name = feature_names[i]
        sub_model = dec_model[feature_name]
        feature = features[i].loc[:, "feature"]
        out[feature_name] = pd.Series()
        print("\033[F", end="")
        print("\033[F", end="")
        print("\r", " " * 40, "\r", end="", flush=True)
        print("gen intermediate data for", feature_name)
        batches = classification.gen_batches(feature.index, batch_size)
        for batch in batches:
            input_data = feature.loc[batch].to_numpy()
            input_data = np.stack(input_data, axis=0)
            input_data = tf.convert_to_tensor(input_data)
            batch_out = sub_model.predict(input_data)
            batch_out = pd.Series(batch_out.tolist())
            out[feature_name] = pd.concat((out[feature_name], batch_out)).reset_index(drop=True)
    return out

def inter_data_concat(inter_data: dict[str, pd.Series],
                      ) -> np.ndarray:
    """
    Takes the result of gen_intermediate_train_data, a dict of intermediate data for explanation
    use, and concatenates the features into a single column of numpy arrays using the additive
    merge method from classification.
    Argument:
    - inter_data: result of gen_intermediate_train_data
    """
    out = pd.Series()
    for feature in inter_data:
        out = classification.additive_merge(out, inter_data[feature])
    out = out.to_numpy()
    print("start merge", datetime.now())
    return np.array([np.concatenate(row) for row in out])

def sub_inter_data(features: list[pd.DataFrame],
                   feature_names: list[str],
                   inter_data: dict[str, pd.Series]
                   ) -> list[pd.DataFrame]:
    """
    Replaces the data in the "feature" column of the input feature datframe list with intermediate
    data generated by gen_intermediate_train_data. The list of feature dataframes will also be
    returned for a functional style.
    Arguments:
    - features: list of feature dataframes where the feature column will be replaced
    - feature_names: list of string feature names, corresponding to the feature dataframes
    - inter_data: result of gen_intermediate_train_data
    """
    for i in range(len(features)):
        feature_name = feature_names[i]
        features[i]["feature"] = inter_data[feature_name]
    return features

def prep_train_data_sample(inter_data: dict[str, pd.Series],
                           features: list[pd.DataFrame],
                           feature_names: list[str],
                           labels: pd.DataFrame,
                           train_data_limit: int,
                           subset_size: int
                           ) -> tuple[np.ndarray, np.ndarray]:
    """
    Prepares input features and intermediate training data for use with the lime tabular explainer.
    Due to memory use limitations, a subset of the complete training data will be extracted and
    returned for later use in the explainer.
    - inter_data: intermediate training data generated with gen_intermediate_train_data
    - features: list of feature dataframes
    - feature_names: list of string feature names, corresponding to the feature dataframes
    - labels: dataframe containing labels for the samples in the features dataframes
    - subset_size: size of the randomly sampled subset of the complete features dataframe
    """
    sub_data = sub_inter_data(features,
                              feature_names,
                              inter_data)
    del inter_data
    joined = pd.DataFrame()
    subset = labels.loc[0:train_data_limit-1].sample(subset_size).index
    print("preparing train data subset")
    for i in subset:
        sample = labels.loc[i, "name"]
        label = labels.loc[i, "label"]
        print("\r", " " * 40, "\r", end="", flush=True)
        print("adding: ", sample, "label: ", label, datetime.now(), end="", flush=True)
        new_sample = pd.DataFrame([])
        for i in range(len(sub_data)):
            temp = sub_data[i].loc[sub_data[i]["sample"] == sample].reset_index(drop=True)
            temp = pd.DataFrame({feature_names[i]: temp["feature"]})
            new_sample = classification.additive_merge(new_sample, temp)
        new_sample["label"] = label
        new_sample = new_sample.reset_index(drop=True)
        joined = pd.concat((joined, new_sample))
    out_labels = joined["label"].to_numpy()
    joined.drop(columns=["label"])
    joined_features_map = {}
    for name in feature_names:
        joined_features_map[name] = joined[name]
        joined.drop(columns=[name])
    return inter_data_concat(joined_features_map), out_labels

def make_explainer(labels: pd.DataFrame,
                   model: Model,
                   features: list[pd.DataFrame],
                   feature_names: list[str],
                   batch_size: int,
                   train_data_limit: int,
                   subset_size: int,
                   cache: bool = True,
                   use_cached: bool = True,
                   cache_name: str = "explainer"
                   ) -> lt.LimeTabularExplainer:
    """
    Returns a tabular explainer generated with a random subset of the input feature data.
    Arguments:
    - labels: dataframe containing labels for the samples in the features dataframes
    - model: composite model as generated using the function in networks
    - features: list of feature dataframes
    - feature_names: list of string feature names, corresponding to the feature dataframes
    - batch_size: maximum batch size of the intermediate feature generation
    - subset_size: size of the randomly sampled subset of the complete features dataframe
    Keyword Arguments:
    - cache: when True, data generated by the intermediate feature generation will be cached
    - use_cached: when True, previously cached data will be used
    - cache_name: name to give the cache
    """
    if use_cached and os.path.isfile("cache/explainers/" + cache_name):
        check_cache()
        train_subset, labels_subset = pickle.load(open("cache/explainers/" + cache_name, "rb"))
    else:
        inter_data = gen_intermediate_train_data(model,
                                                 features,
                                                 feature_names,
                                                 batch_size)
        train_subset, labels_subset = prep_train_data_sample(inter_data,
                                                             features,
                                                             feature_names,
                                                             labels,
                                                             train_data_limit,
                                                             subset_size)
    explainer = lt.LimeTabularExplainer(training_data=train_subset,
                                        training_labels=labels_subset,
                                        feature_names=["|" + str(i) + "|" for i in range(218)],
                                        mode="regression")
    if cache:
        check_cache()
        pickle.dump((train_subset, labels_subset),
                    open("cache/explainers/" + cache_name, "wb"))
    return explainer

def explain_single_feature(model_slice: Model,
                           train_feature: pd.DataFrame,
                           samples: pd.Series):
    """
    Not fully implemented
    """
    x = train_feature.loc[:, "feature"].to_numpy()
    x = np.stack(x, axis=0)
    explainer = lt.LimeTabularExplainer(x)
    explanations = []

def isolate_sample(features: list[pd.DataFrame],
                   sample: str
                   ) -> list[pd.DataFrame]:
    """
    Interface for the isolate feature function in classification due to compatability reasons
    """
    return classification.isolate_sample(features, sample)

def format_explanation(explanation: lt.explanation.Explanation
                       ) -> dict[int, float]:
    """
    Formats the output of the lime tabular explainer for summarization. The feature numbers and
    weights are isolated and formatted in a dict {feature number: weight ... }
    Argument:
    - explanation: Output of an instance explanation of the lime tabular explainer
    """
    out = {}
    exp_list = explanation.as_list()
    for val in exp_list:
        feature = int(val[0].split("|")[1])
        result = val[1]
        out[feature] = result
    return out

def explain(model: Model,
            explainer: lt.LimeTabularExplainer,
            features: list[pd.DataFrame],
            feature_names: list[str],
            batch_size: int
            ) -> dict[int, float]:
    """
    Generates an explanation summarized by input feature from given sample data and explainer.
    Arguments:
    - model: black box model to be explained (not decomposed)
    - explainer: Lime tabular explainer generated from make_explainer
    - features: list of dataframes of features of the *sample* to explain
    - feature_names: list of feature name strings
    - batch_size: batch size of intermediate data generation
    """
    out = {}
    summary = {}
    dec_model = networks.decompose(model)
    terminus = dec_model["terminus"]
    terminus.summary()
    print(terminus.input_spec)
    feature_sizes = {}
    for m_name in dec_model:
        feature_sizes[m_name] = dec_model[m_name].output_shape[1]
    del dec_model["terminus"]
    inter_data = gen_intermediate_train_data(model,
                                             features,
                                             feature_names,
                                             batch_size)
    inter_data = inter_data_concat(inter_data)
    print("N = ", len(inter_data))
    for i in range(len(inter_data)):
        print("i =", i)
        exp = explainer.explain_instance(data_row=inter_data[i],
                                         predict_fn=terminus.predict)
        exp = format_explanation(exp)
        for key in exp:
            if key not in out.keys():
                out[key] = []
            out[key].append(exp[key])
        print("\033[F", end="")
        print("\033[F", end="")
    for key in out:
        out[key] = np.average(out[key])
    pos = 0
    for i in range(len(features)):
        name = feature_names[i]
        size = feature_sizes[name]
        summary[name] = []
        for j in range(pos, pos + size):
            if j in out.keys():
                summary[name].append(out[j])
        pos += size
    for name in summary:
        if len(summary[name]) != 0:
            summary[name] = np.average(summary[name])
        else:
            summary[name] = 0
    norm = np.sqrt(np.sum([x ** 2 for x in summary.values()]))
    # norm = 1
    for name in summary:
        summary[name] = summary[name] / norm
    return summary
